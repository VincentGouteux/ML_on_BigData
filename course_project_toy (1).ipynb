{"cells":[{"cell_type":"markdown","source":["# Course project - Machine Learning on Big Data \n***\nIn the following, we perform various implementations of gradient descent from [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf) by Sebastian Ruder (2017)\n\nIn this project, our main objective is to solve a **linear regression** problem by minimizing the Mean Square Loss (MSE) between the model predictions $h_{\\omega}(x^{(i)})$ and the ground truth $y^{(i)}$ :\n\n$$ \\mathcal{J}_{MSE} = \\dfrac{1}{n} \\sum_{i=1}^{n} ( h_{\\omega}(x^{(i)}) - y^{(i)})^{2}$$\n\nwhere : \n* $\\mathcal{D} = \\{x^{(i)} ; y^{(i)}\\}_{i=1}^{n}$ is the training dataset\n* $\\omega$ are the weights of the model $\\in \\mathbb{R}^{1xd}$\n* $h_{\\omega}$ is a linear approximator : $h_{\\omega}(x^{(i)}) = \\sum_{i=1}^{d} \\omega_i x_{j}^{(i)} = \\omega^{T} x_{j}$\n#### <font color=blue>  Students : Vincent Gouteux & Louis Monier </font>"],"metadata":{}},{"cell_type":"code","source":["#import findspark \n#findspark.init() \nimport pyspark\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom sklearn.datasets import load_boston, load_diabetes\nfrom IPython.display import Image"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["sc = pyspark.SparkContext(appName=\"Course Project\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## 0 - Data preparation\n***\n* We first used a simple toy dataset to perform linear regression and easily check our implementations. Concretely, we generate n data points sampled from a Gaussian distribution $\\mathcal{N}(0,1)$. Noise is added so optimization techniques do not converge too fast."],"metadata":{}},{"cell_type":"code","source":["n = 1000 # number of examples\nd = 2 # number of features"],"metadata":{"scrolled":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["bias = np.ones((n,1)) \nX = np.random.normal(loc=0.0, scale=1.0, size=(n,1)) # generate n points from Gaussian distribution\nnoise = np.random.normal(loc=0.0, scale=1.0, size=(n,1)) # add noise \n\ny = 5 * X + 2 + noise # simple toy dataset\nX = np.hstack((bias, X)) \ndata = np.hstack((X, y))\nw_star = np.dot(np.linalg.pinv(X), y).T\nprint(\"Number of examples : \", n)\nprint(\"Number of features : \", d)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of examples :  1000\nNumber of features :  2\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## 1 - Warm up with Vanilla Gradient Descent (aka Batch Gradient Descent)\n\nVanilla Gradient Descent (Vanilla GD) computes the gradient of the loss w.r.t. to the parameters θ for the entire training dataset:\n\n$$ \\omega = \\omega - \\eta . \\nabla{\\omega} \\mathcal{J}(\\omega) $$\n\nWe mention once and for all (here in the case of Vanilla GD), the derivation of the gardient : \n$$ \\nabla{\\omega} \\mathcal{J}_{MSE} = \\dfrac{2}{n} \\sum_{i=1}^{n} ( h_{\\omega}(x^{(i)}) - y^{(i)}) . x^{(i)} $$\n***"],"metadata":{}},{"cell_type":"code","source":["w = np.random.randn(1,d)\nhistory_GD = []\neta = 1e-2 # step-size \nnb_iter = 200\n\nprint(\"* Start training..\")\nfor i in range(nb_iter):\n    rdd = sc.parallelize(data)\n    rdd = rdd.map(lambda x : 2 * ( np.dot(w, x[:-1]) - x[-1] ) * x[:-1])\n    rdd = rdd.reduce(lambda x,y : (x+y)) / n\n    w -= eta * rdd\n\n    mse = np.linalg.norm(w - w_star)\n    history_GD.append(mse)\n    \n    if (i%20 == 0) :\n        print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(i, nb_iter, mse))\nprint(\"* End training..\")"],"metadata":{"scrolled":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">* Start training..\nIter : [0/200] ; MSE = 3.368\nIter : [20/200] ; MSE = 2.202\nIter : [40/200] ; MSE = 1.440\nIter : [60/200] ; MSE = 0.941\nIter : [80/200] ; MSE = 0.616\nIter : [100/200] ; MSE = 0.403\nIter : [120/200] ; MSE = 0.264\nIter : [140/200] ; MSE = 0.173\nIter : [160/200] ; MSE = 0.113\nIter : [180/200] ; MSE = 0.074\n* End training..\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(history_GD)\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\nplt.title(\"Vanilla Gradient Descent\")\nplt.show()"],"metadata":{"scrolled":false},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## 2 - Mini-batch Gradient Descent\n***\nMini-batch gradient descent performs an update for every mini-batch of n training examples:\n\n\n$$ \\omega = \\omega - \\eta . \\nabla{\\omega} \\mathcal{J}(\\omega ; x^{(i:i+n)} ; y^{(i:i+n)}) $$"],"metadata":{}},{"cell_type":"code","source":["# PARTITION WE ARE GOING TO USE FOR ALL GD METHOD\n\nnb_repart = 20 # number of batch  \nassert(n > nb_repart)\nbatch_size = n // nb_repart\nprint(\"Batch size :\", batch_size)\nnb_iter = 10\n\nrdd = sc.parallelize(data)\nrdd = rdd.repartition(nb_repart).cache() # divide the data in \"nb_repart\" number of partitions\nrdd = rdd.glom().zipWithIndex() # coalescing all elements within each partition into a list"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Batch size : 50\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["w = np.random.randn(1,d)\nhistory_BGD = []\neta = 1e-2\n\n    \nprint(\"* Start training..\")\nfor i in range(nb_iter):\n    for j in range(nb_repart):\n        batch = rdd.filter(lambda x: x[1] == j)\n        batch = batch.flatMap(lambda x: x[0])\n        n_in_batch = batch.count()\n        if (n_in_batch > 0):\n            batch = batch.map(lambda x: 2 * ( np.dot(w, x[:-1]) - x[-1] ) * x[:-1])\n            batch = batch.reduce(lambda a,b: (a+b)) / n_in_batch\n            w -= eta* batch\n            mse = np.linalg.norm(w - w_star)\n            history_BGD.append(mse)\n        else:\n            print(\"Empty RDD..\")\n\n    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(i, nb_iter, mse)) \nprint(\"* End training..\")"],"metadata":{"scrolled":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">* Start training..\nIter : [0/10] ; MSE = 3.446\nIter : [1/10] ; MSE = 2.240\nIter : [2/10] ; MSE = 1.456\nIter : [3/10] ; MSE = 0.948\nIter : [4/10] ; MSE = 0.618\nIter : [5/10] ; MSE = 0.403\nIter : [6/10] ; MSE = 0.264\nIter : [7/10] ; MSE = 0.174\nIter : [8/10] ; MSE = 0.115\nIter : [9/10] ; MSE = 0.077\n* End training..\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(history_BGD)\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Loss\")\nplt.title(\"Mini-batch Gradient Descent\")\nplt.show()"],"metadata":{"scrolled":false},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## 3 - Stochastic Gradient Descent (SGD)\n***\nWe tried several methods to perform SGD. The first one was to create as many repartitions as points in the dataset. We observed the repartitions were not uniform which resulted in a lot of of empty useless RDDs. Moreover, even if we dealt with empty RDDs, the running time was very long, which is the opposite of what SGD was designed for. \n\nWe then decided to move on and implement SGD using some tricks. This approach does not exploit as much the benefits of the Map Reduce framework but it works properly. As many optimizers in the following are based on SGD, we decided to keep this version to have a stable, well-performing reference and show relevant comparisons. To be as thorough as possible, we also implement all the following gradient descent variants based on classic gradient descent.\n\nWe recall the gradient update rule for SGD which consists to uniformaly-at-random select an example from the dataset $\\{x^{(i)} ; y^{(i)}\\}$ (instead of the entire dataset for Vanilla GD or a subset of the dataset for Batch GD) and evaluate the gradient : \n\n$$ \\omega = \\omega - \\eta . \\nabla{\\omega} \\mathcal{J}(\\omega ; x^{(i)} ; y^{(i)}) $$"],"metadata":{}},{"cell_type":"code","source":["w = np.random.randn(1,d)\nhistory_SGD = []\neta = 1e-2\nprint(\"Batch size :\", batch_size)\nnb_iter = 10\nprint(\"* Start training..\")\nfor j in range(nb_repart):\n    batch = rdd.filter(lambda x: x[1] == j)\n    batch = batch.flatMap(lambda x: x[0])\n    n_in_batch = batch.count()\n    if (n_in_batch > 0):\n      for k,r in enumerate(batch.take(n_in_batch)) : \n        x = r[:2]\n        y = r[2:]\n        grad = 2*x*(np.dot(w,x) - y )\n        w = w - eta*grad\n        mse = np.linalg.norm(w-w_star)\n        history_SGD.append(mse)\n    else:\n        print(\"Empty RDD..\")\n\n    if (j%4 == 0) :\n      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \nprint(\"* End training..\")"],"metadata":{"scrolled":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Batch size : 50\n* Start training..\nIter : [0/20] ; MSE = 1.603\nIter : [4/20] ; MSE = 0.169\nIter : [8/20] ; MSE = 0.116\nIter : [12/20] ; MSE = 0.137\nIter : [16/20] ; MSE = 0.138\n* End training..\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(history_GD, label = 'GD')\nplt.plot(history_BGD, label = 'BGD')\nplt.plot(history_SGD[:200], label = 'SGD')\n\n\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\nplt.title('Summary/comparison of different gradient descent methods')\nplt.legend();"],"metadata":{"scrolled":false},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Here we sums up pros and cons of basic gradient methods seen above (based on the [article](https://arxiv.org/pdf/1609.04747.pdf)) : \n* Vanilla GD can be very slow and is intractable for datasets that do not fit in memory. Here we used a toy training set so the method works and show good convergence.\n* Vanilla GD is redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update\n* SGD does away with this redundancy by performing one update at a time. It is much faster than vanilla GD and can be used online. However, SGD has a much high variance.\n* Mini-batch GD takes the best of both worlds. It is a trade-off between vanilla GD and SGD. \n\nNow, let's review some more sophisticated gradient descent optimization algorithms."],"metadata":{}},{"cell_type":"markdown","source":["## 4 - Momentum\n***\nSGD is subject to oscillations during training. Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. The method consists of adding a fraction $\\gamma$ of the update vector of the past time step to the current update vector :\n\n$$ v_t = \\gamma v_{t-1} + \\eta  \\nabla{\\omega} \\mathcal{J}(\\omega) $$\n$$ \\omega = \\omega - v_t $$"],"metadata":{}},{"cell_type":"code","source":["w = np.zeros((1,d))\nhistory_momentum = []\nv_prev = 0. # initialization\nw = np.random.randn(1,d)\neta = 1e-2\ngamma = 0.8\nprint(\"Batch size :\", batch_size)   \nprint(\"* Start training..\")\nfor j in range(nb_repart):\n    batch = rdd.filter(lambda x: x[1] == j)\n    batch = batch.flatMap(lambda x: x[0])\n    n_in_batch = batch.count()\n    if (n_in_batch > 0):\n      for k,r in enumerate(batch.take(n_in_batch)) : \n        x = r[:2]\n        y = r[2:]\n        grad = 2*x*(np.dot(w,x) - y )\n        if (k == 0) :\n          vlast = 0\n          v = eta*grad\n        else :\n          v = gamma*vlast + eta*grad\n          vlast = v\n        w = w - v\n        if (i%10 == 0 ) :\n          print(w)\n        mse = np.linalg.norm(w-w_star)\n        history_momentum.append(mse)\n    else:\n        print(\"Empty RDD..\")\n    if (j%4 == 0) :\n      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \nprint(\"* End training..\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Batch size : 50\n* Start training..\nIter : [0/20] ; MSE = 0.535\nIter : [4/20] ; MSE = 0.084\nIter : [8/20] ; MSE = 0.242\nIter : [12/20] ; MSE = 0.331\nIter : [16/20] ; MSE = 0.165\n* End training..\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(history_SGD[:200], label = 'SGD')\nplt.plot(history_momentum[:200], label = 'SGD Momentum')\n\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\nplt.title('Comparison MSE SGD vs GD Momentum')\nplt.legend();"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["As the gamma parameter seems to play an important role in the momentum gradient descent, we decided to perform hyperparameter tuning to estimate optimal range of values."],"metadata":{}},{"cell_type":"code","source":["### MOMENTUM ### #finding best gamma\nhistory_gamma = []\nfor j in range (10): \n    gamma = 0.1*j\n    w = np.zeros(2)\n    step = 0.02 \n    print(\"Gamma = \", gamma)\n    for j in range(nb_repart):\n        batch = rdd.filter(lambda x: x[1] == j)\n        batch = batch.flatMap(lambda x: x[0])\n        n_in_batch = batch.count()\n        if (n_in_batch > 0):\n          for k,r in enumerate(batch.take(n_in_batch)) : \n            x = r[:2]\n            y = r[2:]\n            grad = 2*x*(np.dot(w,x) - y )\n            if (k == 0) :\n              vlast = 0\n              v = eta*grad\n            else :\n              v = gamma*vlast + eta*grad\n              vlast = v\n            w = w - v\n            if (i%10 == 0 ) :\n              print(w)\n            mse = np.linalg.norm(w-w_star)\n            history_gamma.append(mse)\n        else:\n            print(\"Empty RDD..\")\nprint(len(history_gamma))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Gamma =  0.0\nGamma =  0.1\nGamma =  0.2\nGamma =  0.30000000000000004\nGamma =  0.4\nGamma =  0.5\nGamma =  0.6000000000000001\nGamma =  0.7000000000000001\nGamma =  0.8\nGamma =  0.9\n10000\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(history_gamma[:200], label = 'Gamma = 0.0')\nplt.plot(history_gamma[1000:1200], label = 'Gamma = 0.1')\nplt.plot(history_gamma[2000:2200], label = 'Gamma = 0.2')\nplt.plot(history_gamma[3000:3200], label = 'Gamma = 0.3')\nplt.plot(history_gamma[4000:4200], label = 'Gamma = 0.4')\nplt.plot(history_gamma[5000:5200], label = 'Gamma = 0.5')\nplt.plot(history_gamma[6000:6200], label = 'Gamma = 0.6')\nplt.plot(history_gamma[7000:7200], label = 'Gamma = 0.7')\nplt.plot(history_gamma[8000:8200], label = 'Gamma = 0.8')\nplt.plot(history_gamma[9000:9200], label = 'Gamma = 0.9')\nplt.title('MSE for different gradient descent momentum gammas')\nplt.legend()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## 5 -  Nesterov accelerated gradient (NAG)\n***\nNAG is a way to give our momentum term this kind of prescience. We know that we will use our momentum term γvt−1 to move the parameters $\\theta$. Computing $\\theta -  \\gamma v_{t-1}$ thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be :\n\n$$ v_t = \\gamma v_{t-1} + \\eta . \\nabla{\\omega} \\mathcal{J}(\\omega - \\gamma v_{t-1}) $$\n$$ \\omega = \\omega - v_t $$"],"metadata":{}},{"cell_type":"code","source":["w = np.zeros((1,d))\nhistory_nesterov = []\nv_prev = 0. # initialization\neta = 1e-2\ngamma = 0.8\nprint(\"Batch size :\", batch_size)\nprint(\"* Start training..\")\nfor j in range(nb_repart):\n    batch = rdd.filter(lambda x: x[1] == j)\n    batch = batch.flatMap(lambda x: x[0])\n    n_in_batch = batch.count()\n    if (n_in_batch > 0):\n      for k,r in enumerate(batch.take(n_in_batch)) : \n        x = r[:2]\n        y = r[2:]\n        if (i == 0) :\n          vlast = 0\n          grad = 2*x*(np.dot(w,x) - y )\n          v = step*grad\n        else :\n          grad = 2*x*(np.dot(w-v,x) - y )\n          v = gamma*vlast + step*grad\n          vlast = v\n        w = w - v\n        if (i%10 == 0 ) :\n          print(w)\n        mse = np.linalg.norm(w-w_star)\n        #print(mse)\n        history_nesterov.append(mse)\n    else:\n        print(\"Empty RDD..\")\n    if (j%4 == 0) :\n      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \nprint(\"* End training..\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Batch size : 50\n* Start training..\nIter : [0/20] ; MSE = 0.786\nIter : [4/20] ; MSE = 0.254\nIter : [8/20] ; MSE = 0.298\nIter : [12/20] ; MSE = 0.295\nIter : [16/20] ; MSE = 0.244\n* End training..\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(history_GD, label = 'SGD ')\nplt.plot(history_momentum[:200], label = 'SGD Momentum')\nplt.plot(history_nesterov[:200], label = 'SGD Nesterov ')\n\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\nplt.title('Comparison MSE SGD vs GD Momentum')\nplt.legend();"],"metadata":{"scrolled":false},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## 6 - Adagrad\n***\nAdagrad adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. The gradient update rule is given by the following expression :\n\n$$ \\omega_{t+1} = \\omega_{t} - \\dfrac{\\eta}{\\sqrt(G_T + \\epsilon)} \\odot g_t $$"],"metadata":{}},{"cell_type":"code","source":["w = np.zeros((1,d))\nhistory_adagrad = []\nsum_grad = np.zeros((1,d))\neta = 0.7\ngamma = 0.9 # classic value\nnb_iter = 200\nepsilon = 1e-8\nprint(\"Batch size :\", batch_size)\nprint(\"* Start training..\")\nfor j in range(nb_repart):\n    batch = rdd.filter(lambda x: x[1] == j)\n    batch = batch.flatMap(lambda x: x[0])\n    n_in_batch = batch.count()\n    if (n_in_batch > 0):\n      for k,r in enumerate(batch.take(n_in_batch)) : \n        x = r[:2]\n        y = r[2:]\n        grad = 2*x*(np.dot(w,x) - y )\n        sum_grad += grad**2\n        adjusted_eta = eta / np.sqrt(epsilon + sum_grad)\n        w -= np.multiply(adjusted_eta, grad) # element-wise \n    \n        mse = np.linalg.norm(w - w_star)\n        history_adagrad.append(mse)  \n        if (i%10 == 0 ) :\n          print(w)\n    else:\n        print(\"Empty RDD..\")\n    if (j%4 == 0) :\n      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \nprint(\"* End training..\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Batch size : 50\n* Start training..\nIter : [0/20] ; MSE = 0.355\nIter : [4/20] ; MSE = 0.183\nIter : [8/20] ; MSE = 0.108\nIter : [12/20] ; MSE = 0.154\nIter : [16/20] ; MSE = 0.139\n* End training..\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(history_GD, label = 'SGD ')\nplt.plot(history_momentum[:200], label = 'SGD Momentum')\nplt.plot(history_nesterov[:200], label = 'SGD Nesterov ')\nplt.plot(history_adagrad[:200], label = 'SGD Adagrad ')\n\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\nplt.title('Comparison')\nplt.legend();"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Adagrad is working fine, however we noticed that we needed a step much larger than for momentum and Nesterov. Changing the step size $\\eta$ makes the comparison between methods harder. However, if we select such value for Adagrad, the curve is smoother and we reach approximately the same convergence."],"metadata":{}},{"cell_type":"markdown","source":["## 7 -  Adadelta\n***\nIt is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients : the sum of gradients is recursively defined as a decaying average of all past squared gradients. \n\n$$ \\omega_{t+1} = \\omega_{t} - \\dfrac{RMS[\\Delta \\omega]_{t+1}}{RMS[g]_{t}} g_t $$\n$$ \\omega_{t+1} = \\omega_{t} + \\Delta \\omega_t $$"],"metadata":{}},{"cell_type":"code","source":["w = np.zeros((1,d))\nhistory_adadelta = []\ndelta = np.zeros((1,d))\nEg_prev = 0.\nEd_prev = 0.\nRMSd = 0.\n\neta = 3e-3\ngamma = 0.9 # classic value\nnb_iter = 200\nepsilon = 1e-8\n\nprint(\"Batch size :\", batch_size)\n\nprint(\"* Start training..\")\nfor j in range(nb_repart):\n    batch = rdd.filter(lambda x: x[1] == j)\n    batch = batch.flatMap(lambda x: x[0])\n    n_in_batch = batch.count()\n    if (n_in_batch > 0):\n      for k,r in enumerate(batch.take(n_in_batch)) : \n        x = r[:2]\n        y = r[2:]\n        grad = 2*x*(np.dot(w,x) - y ) /n\n        \n        Eg = gamma * Eg_prev + (1-gamma) * grad**2\n        RMSg = np.sqrt(Eg  + epsilon)\n\n        w -= (RMSd / RMSg) * grad\n\n        ###\n        delta -= (eta / RMSg) * grad\n        Ed = gamma * Eg_prev + (1-gamma) * delta**2\n        RMSd = np.sqrt(Ed  + epsilon)\n\n        Eg_prev = Eg\n        Ed_prev = Ed\n\n        mse = np.linalg.norm(w - w_star)\n        history_adadelta.append(mse)\n    else:\n        print(\"Empty RDD..\")\n    if (j%4 == 0):\n      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \nprint(\"* End training..\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Batch size : 50\n* Start training..\nIter : [0/20] ; MSE = 4.410\nIter : [4/20] ; MSE = 0.133\nIter : [8/20] ; MSE = 0.137\nIter : [12/20] ; MSE = 0.242\nIter : [16/20] ; MSE = 0.187\n* End training..\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(history_adadelta[:200])\n\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\nplt.title('Adadelta');"],"metadata":{"scrolled":false},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["## 8 - RMSprop\n***\nRMSprop as well divides the learning rate by an exponentially decaying average of squared gradients :\n\n$$ E[{g^2}]_t = 0.9 E[{g^2}]_{t-1} + 0.1 g_{t}^2 $$\n$$ \\omega_{t+1} = \\omega_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t $$"],"metadata":{}},{"cell_type":"code","source":["w = np.zeros((1,d))\nhistory_RMS = []\ndelta = np.zeros((1,d))\nEg_prev = 0.\nEd_prev = 0.\n\neta = 3e-2\ngamma = 0.9 # classic value\nnb_iter = 200\nepsilon = 1e-8\n\nprint(\"* Start training..\")\nfor j in range(nb_repart):\n  batch = rdd.filter(lambda x: x[1] == j)\n  batch = batch.flatMap(lambda x: x[0])\n  n_in_batch = batch.count()\n  if (n_in_batch > 0):\n    for k,r in enumerate(batch.take(n_in_batch)) : \n      x = r[:2]\n      y = r[2:]\n      grad = 2*x*(np.dot(w,x) - y )/n\n\n      ###\n      Eg = gamma * Eg_prev + (1-gamma) * grad**2\n      RMSg = np.sqrt(Eg  + epsilon)\n\n      w -= (eta / RMSg) * grad\n\n      Eg_prev = Eg\n\n      mse = np.linalg.norm(w - w_star)\n      history_RMS.append(mse)\n  if (j%4 == 0): \n    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \n\nprint(\"* End training..\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">* Start training..\nIter : [0/20] ; MSE = 3.994\nIter : [4/20] ; MSE = 0.262\nIter : [8/20] ; MSE = 0.126\nIter : [12/20] ; MSE = 0.183\nIter : [16/20] ; MSE = 0.151\n* End training..\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(history_adadelta[:300], label = 'Adadelta')\nplt.plot(history_RMS[:300], label = ' RMSProp')\n\nplt.legend()\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\nplt.title('RMSprop');"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["## 9 - Adaptive Moment Estimation (Adam)\n***\nAnother method that computes adaptive learning rates for each parameter :\n\n$$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$\n$$ v_t = \\beta_2 v_{t-1} +(1 - \\beta_2) g_t^2 $$\n$$ \\omega_{t+1} = \\omega_t - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t $$"],"metadata":{}},{"cell_type":"code","source":["w = np.zeros((1,d))\nhistory_adam = []\ndelta = np.zeros((1,d))\nmom_prev = 0.\nv_prev = 0.\n\neta = 5e-3\nnb_iter = 200\nepsilon = 1e-8\nbeta1 = 0.9\nbeta2 = 0.999\n\nprint(\"* Start training..\")\nfor j in range(nb_repart):\n  batch = rdd.filter(lambda x: x[1] == j)\n  batch = batch.flatMap(lambda x: x[0])\n  n_in_batch = batch.count()\n  if (n_in_batch > 0):\n    for k,r in enumerate(batch.take(n_in_batch)) : \n      x = r[:2]\n      y = r[2:]\n      grad = 2*x*(np.dot(w,x) - y )/n\n      mom = beta1 * mom_prev + (1-beta1) * grad \n      v = beta2 * v_prev + (1-beta2) * grad**2\n\n      avg_mom = mom / (1 - beta1)\n      avg_v = v / (1 - beta1)\n\n      w -= (eta / np.sqrt(avg_v)) * avg_mom\n\n      mse = np.linalg.norm(w - w_star)\n      history_adam.append(mse)\n      \n      v_prev = v\n      mom_prev = mom \n  if (j%4 == 0 ):\n    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \n\nprint(\"* End training..\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">* Start training..\nIter : [0/20] ; MSE = 2.138\nIter : [4/20] ; MSE = 0.252\nIter : [8/20] ; MSE = 0.155\nIter : [12/20] ; MSE = 0.104\nIter : [16/20] ; MSE = 0.099\n* End training..\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(history_adam[:300], label = 'Adam')\nplt.plot(history_adadelta[:300], label = 'Adadelta')\nplt.plot(history_RMS[:300], label = 'RMSProp')\n\nplt.legend()\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\nplt.title('Adam');"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["## 10 - AdaMax\n***\nIt is a generalization of the $v_t$ update with the $l_{\\infty}$ norm :\n\n\n$$ u_t = \\beta_2^{\\infty} v_{t-1} +(1 - \\beta_2^{\\infty}) \\mid g_t\\mid^{\\infty} = max(\\beta_2 . v_{t-1}, \\mid g_t \\mid) $$\n$$ \\omega_{t+1} = \\omega_t - \\dfrac{\\eta}{u_t} \\hat{m}_t $$"],"metadata":{}},{"cell_type":"code","source":["w = np.zeros((1,d))\nhistory_adamax = []\ndelta = np.zeros((1,d))\nmom_prev = 0.\nu_prev = 0.\n\neta = 1e-3\nnb_iter = 200\nepsilon = 1e-8\nbeta1 = 0.9\nbeta2 = 0.999\nprint(\"* Start training..\")\nfor j in range(nb_repart):\n  batch = rdd.filter(lambda x: x[1] == j)\n  batch = batch.flatMap(lambda x: x[0])\n  n_in_batch = batch.count()\n  if (n_in_batch > 0):\n    for k,r in enumerate(batch.take(n_in_batch)) : \n      x = r[:2]\n      y = r[2:]\n      grad = 2*x*(np.dot(w,x) - y )/n\n\n      mom = beta1 * mom_prev + (1-beta1) * grad \n      v = beta2 * v_prev + (1-beta2) * grad**2\n\n      avg_mom = mom / (1 - beta1)\n      avg_v = v / (1 - beta1)\n      u = np.maximum(beta2 * v_prev, np.abs(grad))\n\n      avg_mom = mom / (1 - beta1)\n      avg_v = v / (1 - beta1)\n      w -= (eta / u) * avg_mom\n      \n      v_prev = v\n      mom_prev = mom \n\n      mse = np.linalg.norm(w - w_star)\n      history_adamax.append(mse)\n  if ( j%4 == 0):\n    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse))\nprint(\"* End training..\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">* Start training..\nIter : [0/20] ; MSE = 2.912\nIter : [4/20] ; MSE = 0.271\nIter : [8/20] ; MSE = 0.368\nIter : [12/20] ; MSE = 0.085\nIter : [16/20] ; MSE = 0.289\n* End training..\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(history_adamax[:300], label = 'Adamax')\nplt.plot(history_adam[:300], label = 'Adam')\nplt.plot(history_adadelta[:300], label = 'Adadelta')\nplt.plot(history_RMS[:300], label = 'RMSProp')\n\nplt.legend()\n\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\n"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["## 11 - Nadam \n***\nThis is a combination of Adam and Nestorov :\n\n$$ \\Delta \\omega_t = - \\dfrac{RMS[\\Delta \\omega]_{t-1}}{RMS[g]_t} g_t $$\n$$ \\omega_{t+1} = \\omega_t + \\Delta \\omega_t $$"],"metadata":{}},{"cell_type":"code","source":["w = np.zeros((1,d))\nhistory_nadam = []\ndelta = np.zeros((1,d))\nmom_prev = 0.\nu_prev = 0.\n\neta = 5e-2\nnb_iter = 200\nepsilon = 1e-8\nbeta1 = 0.9\nbeta2 = 0.999\nprint(\"* Start training..\")\nfor j in range(nb_repart):\n  batch = rdd.filter(lambda x: x[1] == j)\n  batch = batch.flatMap(lambda x: x[0])\n  n_in_batch = batch.count()\n  if (n_in_batch > 0):\n    for k,r in enumerate(batch.take(n_in_batch)) : \n      x = r[:2]\n      y = r[2:]\n      grad = 2*x*(np.dot(w,x) - y )/n\n\n      mom = beta1 * mom_prev + (1-beta1) * grad \n      v = beta2 * v_prev + (1-beta2) * grad**2\n\n      avg_mom = mom / (1 - beta1)\n      avg_v = v / (1 - beta1)\n      u = np.maximum(beta2 * v_prev, np.abs(grad))\n\n      avg_mom = mom / (1 - beta1)\n      avg_v = v / (1 - beta1)\n      w -= (eta / (np.sqrt(avg_v)+ epsilon)) * (beta1 * avg_mom + ((1-beta1)* grad)/(1-beta1))\n      \n      v_prev = v\n      mom_prev = mom \n\n      mse = np.linalg.norm(w - w_star)\n      history_nadam.append(mse)\n  if (j%4 == 0):\n    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse))\nprint(\"* End training..\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">* Start training..\nIter : [0/20] ; MSE = 0.734\nIter : [4/20] ; MSE = 0.127\nIter : [8/20] ; MSE = 0.229\nIter : [12/20] ; MSE = 0.409\nIter : [16/20] ; MSE = 0.131\n* End training..\n</div>"]}}],"execution_count":44},{"cell_type":"code","source":["figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(history_nadam[:300], label = 'Nadam')\nplt.plot(history_adamax[:300], label = 'Adamax')\nplt.plot(history_adam[:300], label = 'Adam')\nplt.plot(history_adadelta[:300], label = 'Adadelta')\nplt.plot(history_RMS[:300], label = 'RMSProp')\n\nplt.legend()\n\nplt.legend()\n\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\n"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["figure(num=None, figsize=(15, 11), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(history_GD, label = 'GD')\nplt.plot(history_BGD, label = 'BGD')\nplt.plot(history_SGD[:200], label = 'SGD')\nplt.plot(history_momentum[:200], label = 'Momentum')\nplt.plot(history_nesterov[:200], label = 'Nesterov')\nplt.plot(history_adagrad[:200], label = 'Adagrad')\nplt.plot(history_adadelta[:200], label = 'Adadelta')\nplt.plot(history_RMS[:200], label = 'RMS')\nplt.plot(history_adam[:200], label = 'Adam')\nplt.plot(history_adamax[:200], label = 'Adamax')\nplt.plot(history_nadam[:200], label = 'Nadam')\n\n\nplt.xlabel(\"Number of gradient updates\")\nplt.ylabel(\"Mean Square Error\")\nplt.legend()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":47}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3","nbconvert_exporter":"python","file_extension":".py"},"name":"Projet_MLBG_GDImplementations","notebookId":2692408934725147},"nbformat":4,"nbformat_minor":0}
